{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.script import *\n",
    "\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from learn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "gnorm_types = (nn.GroupNorm,)\n",
    "insnorm_types = (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d)\n",
    "norm_types = bn_types + gnorm_types + insnorm_types\n",
    "\n",
    "def cond_init(m:nn.Module, init_func:LayerFunc):\n",
    "    \"Initialize the non-batchnorm layers of `m` with `init_func`.\"\n",
    "    if (not isinstance(m, norm_types)) and (not isinstance(m, nn.PReLU)) and requires_grad(m): \n",
    "        init_default(m, init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'atlas_brain_mr'\n",
    "bs=1\n",
    "model_name='baseline11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "f = data_dict[data_name]\n",
    "train_paths, valid_paths, test1_paths, test2_paths = f()\n",
    "\n",
    "train_ds = MRI_3D_Dataset(*train_paths)\n",
    "valid_ds = MRI_3D_Dataset(*valid_paths)\n",
    "test1_ds = MRI_3D_Dataset(*test1_paths) if test1_paths else None\n",
    "test2_ds = MRI_3D_Dataset(*test2_paths) if test2_paths else None\n",
    "\n",
    "data = DataBunch.create(train_ds=train_ds, valid_ds=valid_ds, bs=bs)\n",
    "test1_dl = DeviceDataLoader(DataLoader(test1_ds), device=data.device) if test1_ds else None\n",
    "test2_dl = DeviceDataLoader(DataLoader(test2_ds), device=data.device) if test2_ds else None\n",
    "\n",
    "# model\n",
    "f = experiment_model_dict[model_name]; m = f()\n",
    "apply_leaf(m, partial(cond_init, init_func= nn.init.kaiming_normal_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='dummy'\n",
    "model_dir='atlas_brain_mr_models'\n",
    "early_stop=False\n",
    "loss_func='bce'\n",
    "epochs=20\n",
    "lr=0.01\n",
    "one_cycle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn\n",
    "early_stop_cb = partial(EarlyStoppingCallback, monitor='dice_score', mode='max', patience=5)\n",
    "save_model_cb = partial(SaveModelCallback, monitor='dice_score', mode='max', every='improvement', \n",
    "                        name=f'best_of_{MODEL_NAME}')\n",
    "reduce_lr_cb = partial(ReduceLROnPlateauCallback, monitor='dice_score', mode='max', patience=0, factor=0.8)\n",
    "csv_logger_cb = partial(CSVLogger, filename=f'logs/{model_dir}/{MODEL_NAME}')\n",
    "\n",
    "if early_stop: \n",
    "    callback_fns = [early_stop_cb, save_model_cb, reduce_lr_cb, csv_logger_cb, ActivationStats]\n",
    "else: \n",
    "    callback_fns = [save_model_cb, csv_logger_cb, ActivationStats]\n",
    "callbacks = [TerminateOnNaNCallback()]    \n",
    "\n",
    "learn = Learner(data=data, model=m, opt_func=partial(optim.Adam, betas=(0.9,0.99), eps=1e-4),\n",
    "                callbacks=callbacks, callback_fns=callback_fns, model_dir=model_dir)\n",
    "learn.loss_func = {'dice':dice_loss, 'bce':BCEWithLogitsFlat(), 'mixed':MixedLoss(10., 2.)}[loss_func] \n",
    "learn.metrics = [dice_score]\n",
    "learn.to_fp16()\n",
    "\n",
    "# schedule\n",
    "b_its = len(data.train_dl)\n",
    "ph1 = (TrainingPhase(epochs*0.5*b_its)\n",
    "        .schedule_hp('lr', (lr/20,lr), anneal=annealing_cos)\n",
    "        .schedule_hp('eps', (1e-4,1e-4), anneal=annealing_no) # for NAN\n",
    "        )\n",
    "ph2 = (TrainingPhase(epochs*0.5*b_its)\n",
    "        .schedule_hp('lr', (lr,lr/1e5), anneal=annealing_cos)\n",
    "        .schedule_hp('eps', (1e-4,1e-4), anneal=annealing_no)\n",
    "        )\n",
    "gs = GeneralScheduler(learn, (ph1,ph2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "for i in np.linspace(0,1,100): \n",
    "    xs.append(annealing_no(1e-4, 1e-8, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5c20651710>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='2143', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-35f075b2b219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_fastai/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_fastai/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if one_cycle:\n",
    "    learn.fit_one_cycle(epochs, max_lr=lr)\n",
    "else:\n",
    "    learn.fit(epochs, lr, callbacks=gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters in learn.mixed_precision.master_params:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_type=2\n",
    "total_norm = 0\n",
    "for p in parameters:\n",
    "    param_norm = p.grad.data.norm(norm_type)\n",
    "    total_norm += param_norm.item() ** norm_type\n",
    "total_norm = total_norm ** (1. / norm_type)\n",
    "\n",
    "total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm=.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_coef = max_norm / (total_norm + 1e-6)\n",
    "if clip_coef < 1:\n",
    "    for p in parameters:\n",
    "        p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24999928934041302"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_type=2\n",
    "total_norm = 0\n",
    "for p in parameters:\n",
    "    param_norm = p.grad.data.norm(norm_type)\n",
    "    total_norm += param_norm.item() ** norm_type\n",
    "total_norm = total_norm ** (1. / norm_type)\n",
    "\n",
    "total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimWrapper over Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.99)\n",
       "    eps: 0.0001\n",
       "    lr: 0.0005064309880274034\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.99)\n",
       "    eps: 0.0001\n",
       "    lr: 0.0005064309880274034\n",
       "    weight_decay: 0\n",
       ").\n",
       "True weight decay: True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = learn.data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 128, 256, 256]), torch.Size([1, 128, 256, 256]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = learn.activation_stats.modules[0](xb.half().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-14748290., device='cuda:7', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf, device='cuda:7', dtype=torch.float16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100663296"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [-0., 0., -0.,  ..., -0., -0., -0.],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [-0., 0., -0.,  ..., -0., 0., -0.],\n",
       "        [0., -0., -0.,  ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.activation_stats.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0104,  0.0020,  0.0079,  0.0029,  0.0037, -0.0115,  0.0043,  0.0133,\n",
       "        -0.0073, -0.0059,  0.0063, -0.0150], device='cuda:0',\n",
       "       dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.activation_stats.modules[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.activation_stats.stats[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21341668, tensor(0.9287)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(t): return (t.mean(), t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down1.down.0.weight\n",
      "(tensor(-0.0051, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.2671, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.down.0.bias\n",
      "(tensor(-0.0008, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0089, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.down.1.weight\n",
      "(tensor(0.2612, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c1.1.weight\n",
      "(tensor(0.2462, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c1.2.weight\n",
      "(tensor(0.0027, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0790, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c1.2.bias\n",
      "(tensor(-0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0034, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c2.1.weight\n",
      "(tensor(0.2505, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c2.2.weight\n",
      "(tensor(0.0017, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0787, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down1.preact_res.c2.2.bias\n",
      "(tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0062, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.down.0.weight\n",
      "(tensor(0.0007, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0799, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.down.0.bias\n",
      "(tensor(0.0028, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0066, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.down.1.weight\n",
      "(tensor(0.2520, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c1.1.weight\n",
      "(tensor(0.2441, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c1.2.weight\n",
      "(tensor(-0.0004, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0559, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c1.2.bias\n",
      "(tensor(0.0003, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0051, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c2.1.weight\n",
      "(tensor(0.2374, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c2.2.weight\n",
      "(tensor(0.0010, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0558, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down2.preact_res.c2.2.bias\n",
      "(tensor(0.0030, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0076, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.down.0.weight\n",
      "(tensor(-0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0555, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.down.0.bias\n",
      "(tensor(-0.0014, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0055, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.down.1.weight\n",
      "(tensor(0.2472, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c1.1.weight\n",
      "(tensor(0.2469, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c1.2.weight\n",
      "(tensor(-0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0396, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c1.2.bias\n",
      "(tensor(-9.9897e-05, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MeanBackward1>), tensor(0.0043, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c2.1.weight\n",
      "(tensor(0.2455, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c2.2.weight\n",
      "(tensor(-3.8683e-05, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MeanBackward1>), tensor(0.0396, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down3.preact_res.c2.2.bias\n",
      "(tensor(0.0012, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0095, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.down.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0396, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.down.0.bias\n",
      "(tensor(-0.0015, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0041, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.down.1.weight\n",
      "(tensor(0.2363, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c1.1.weight\n",
      "(tensor(0.2339, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c1.2.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0282, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c1.2.bias\n",
      "(tensor(1.6332e-05, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MeanBackward1>), tensor(0.0029, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c2.1.weight\n",
      "(tensor(0.2450, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c2.2.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0282, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "down4.preact_res.c2.2.bias\n",
      "(tensor(-0.0015, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0078, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.0.0.weight\n",
      "(tensor(-0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0280, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.0.0.bias\n",
      "(tensor(-0.0014, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0040, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.0.1.weight\n",
      "(tensor(0.2413, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c1.1.weight\n",
      "(tensor(0.2422, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c1.2.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0200, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c1.2.bias\n",
      "(tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0021, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c2.1.weight\n",
      "(tensor(0.2433, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c2.2.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0199, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.1.c2.2.bias\n",
      "(tensor(-0.0007, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0040, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.2.c.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0199, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.2.c.0.bias\n",
      "(tensor(-0.0008, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0035, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "middle.2.c.1.weight\n",
      "(tensor(0.2522, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c1.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0200, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c1.0.bias\n",
      "(tensor(-0.0033, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0052, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c1.1.weight\n",
      "(tensor(0.2463, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c2.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0200, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c2.0.bias\n",
      "(tensor(-0.0041, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0061, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c1.c2.1.weight\n",
      "(tensor(0.2571, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c2.c.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0281, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c2.c.0.bias\n",
      "(tensor(-0.0047, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0084, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock1.c2.c.1.weight\n",
      "(tensor(0.2452, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c1.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0281, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c1.0.bias\n",
      "(tensor(-0.0045, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0056, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c1.1.weight\n",
      "(tensor(0.2379, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c2.0.weight\n",
      "(tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0281, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c2.0.bias\n",
      "(tensor(-0.0067, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0071, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c1.c2.1.weight\n",
      "(tensor(0.2517, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c2.c.0.weight\n",
      "(tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0396, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c2.c.0.bias\n",
      "(tensor(-0.0037, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0060, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock2.c2.c.1.weight\n",
      "(tensor(0.2578, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c1.0.weight\n",
      "(tensor(0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0395, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c1.0.bias\n",
      "(tensor(-0.0025, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0058, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c1.1.weight\n",
      "(tensor(0.2605, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c2.0.weight\n",
      "(tensor(0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0396, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c2.0.bias\n",
      "(tensor(-0.0046, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0073, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c1.c2.1.weight\n",
      "(tensor(0.2615, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c2.c.0.weight\n",
      "(tensor(0.0005, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0554, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c2.c.0.bias\n",
      "(tensor(-0.0012, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0076, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "upblock3.c2.c.1.weight\n",
      "(tensor(0.2678, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg2.c1.0.weight\n",
      "(tensor(-0.0003, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0397, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg2.c1.0.bias\n",
      "(tensor(-0.0075, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0064, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg2.c1.1.weight\n",
      "(tensor(0.2705, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg2.c2.weight\n",
      "(tensor(-0.0093, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.2197, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg2.c2.bias\n",
      "(tensor(-0.0486, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg3.c1.0.weight\n",
      "(tensor(0.0003, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0562, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg3.c1.0.bias\n",
      "(tensor(-0.0059, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0071, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg3.c1.1.weight\n",
      "(tensor(0.2598, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg3.c2.weight\n",
      "(tensor(0.0636, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.2688, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg3.c2.bias\n",
      "(tensor(-0.0486, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg_final.c1.0.weight\n",
      "(tensor(-0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0558, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg_final.c1.0.bias\n",
      "(tensor(-0.0025, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.0063, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg_final.c1.1.weight\n",
      "(tensor(0.2673, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg_final.c2.weight\n",
      "(tensor(-0.0442, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(0.2732, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n",
      "seg_final.c2.bias\n",
      "(tensor(-0.0486, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward1>), tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = learn.model.named_parameters()\n",
    "\n",
    "for n,p in params:\n",
    "    print(n)\n",
    "    print(stats(p))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.relu(x) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(16,16,3),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(16,16,3),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(16,16,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,1,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0185, grad_fn=<MeanBackward1>),\n",
       " tensor(0.0805, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = m(x)\n",
    "stats(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_leaf(m, partial(cond_init, init_func= nn.init.kaiming_normal_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0306, grad_fn=<MeanBackward1>),\n",
       " tensor(1.2393, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = m(x)\n",
    "stats(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
